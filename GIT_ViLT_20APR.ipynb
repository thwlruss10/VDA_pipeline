{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlruss10/VDA_pipeline/blob/main/GIT_ViLT_20APR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zioGKih-t97P",
      "metadata": {
        "id": "zioGKih-t97P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets nltk scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-CQpvTZ_vsSN",
      "metadata": {
        "id": "-CQpvTZ_vsSN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd062d49",
      "metadata": {
        "id": "dd062d49"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models & transformers (ViTModel, DeiTModel, BEiT)\n",
        "    AutoModel,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "# SET ONLY 1 GPU DEVICE\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "#set_caching_enabled(True)>> this line deleted as 'datasets' no longer supports explicit cahce enabling\n",
        "#set_caching_enabled(True)\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gyxWJWH2z6Xc",
      "metadata": {
        "id": "gyxWJWH2z6Xc"
      },
      "source": [
        "Above code imports dependencies, sets cache path for Huggingface transformers, error logs message threshold set to supress warnings deemed not severe. GPU device set for torch processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TF7cDaBPP06j",
      "metadata": {
        "id": "TF7cDaBPP06j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory pointing to your Drive location\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Define regex to extract image ID\n",
        "image_pattern = re.compile(r\"( (in |on |of )?(the |this )?(image\\d*) \\?)\")\n",
        "\n",
        "# Read the raw Q&A file\n",
        "# Extracts image ID and question at index[i] together with answer at [i+1]\n",
        "# and organizes it into a pandas DataFrame, creates a list of unique answers,\n",
        "# and then splits the dataset into training and testing sets, saving them as\n",
        "# CSV.\n",
        "\n",
        "qa_file = os.path.join(base_dir, \"all_qa_pairs.txt\")\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as f: # open in read mode with in utf-8 encoding\n",
        "    qa_data = [x.strip() for x in f.readlines()] # x.strip() removes white spaces\n",
        "\n",
        "records = []\n",
        "for i in range(0, len(qa_data), 2): # iterates in steps of two because quesiton is at index i and answer at i+1\n",
        "    match = image_pattern.findall(qa_data[i]) # extract image ID from question string\n",
        "    if match:\n",
        "        img_id = match[0][3]\n",
        "        question = qa_data[i].replace(match[0][0], \"\").strip() # remove image ID and replace question\n",
        "        answer = qa_data[i + 1].strip() # answer retrieved from next element\n",
        "        records.append({\"question\": question, \"answer\": answer, \"image_id\": img_id}) #populate dictionary\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "\n",
        "answer_space = []\n",
        "for ans in df[\"answer\"].to_list():\n",
        "    if \",\" in ans:\n",
        "        answer_space += ans.replace(\" \", \"\").split(\",\")\n",
        "    else:\n",
        "        answer_space.append(ans)\n",
        "\n",
        "# sort and remove duplicates from answer_space\n",
        "answer_space = sorted(set(answer_space))\n",
        "\n",
        "# Write the answer space to file\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(\"\\n\".join(answer_space))\n",
        "\n",
        "# Split dataset into training and evaluation\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save to CSV\n",
        "train_df.to_csv(os.path.join(base_dir, \"data_train.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(base_dir, \"data_eval.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j9nPozrCG5_O",
      "metadata": {
        "id": "j9nPozrCG5_O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "eval_df = pd.read_csv(os.path.join(base_dir, \"data_eval.csv\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fDYaWz-P3dlp",
      "metadata": {
        "id": "fDYaWz-P3dlp"
      },
      "source": [
        "*** VLiT experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TarXEeJm-zs-",
      "metadata": {
        "id": "TarXEeJm-zs-"
      },
      "outputs": [],
      "source": [
        "from transformers.models.vilt.modeling_vilt import ViltForQuestionAnswering\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ViltForSingleLabelQA(ViltForQuestionAnswering):\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        pixel_values=None,\n",
        "        pixel_mask=None,\n",
        "        labels=None,\n",
        "        return_dict=True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # ðŸ§¼ Remove any Trainer-only args\n",
        "        kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        # âœ… Forward to base ViLT model without passing labels (so we control loss)\n",
        "        outputs = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            pixel_mask=pixel_mask,\n",
        "            return_dict=return_dict,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # âœ… Our own classification loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if labels.dtype != torch.long:\n",
        "                labels = labels.long()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LDo0LYGycZpl",
      "metadata": {
        "id": "LDo0LYGycZpl"
      },
      "outputs": [],
      "source": [
        "def wup_measure(pred, gt, threshold=0.9):\n",
        "    pred_synsets = wn.synsets(pred)\n",
        "    gt_synsets = wn.synsets(gt)\n",
        "\n",
        "    if not pred_synsets or not gt_synsets:\n",
        "        return 0.0\n",
        "\n",
        "    max_score = max((s1.wup_similarity(s2) or 0) for s1 in pred_synsets for s2 in gt_synsets)\n",
        "    return 1.0 if max_score >= threshold else max_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_98Wlq1B3yYR",
      "metadata": {
        "id": "_98Wlq1B3yYR"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load model and processor\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load image and define question\n",
        "image_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image2.png\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "question = \"What is on the left side of the sink?\"\n",
        "\n",
        "# Preprocess inputs\n",
        "inputs = processor(image, question, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_answer = model.config.id2label[logits.argmax(-1).item()]\n",
        "\n",
        "print(\"Predicted Answer:\", predicted_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5pxKmfgr8-gX",
      "metadata": {
        "id": "5pxKmfgr8-gX"
      },
      "source": [
        "Batch processing, Construct classification problem, Model training, output results compare to ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Zg5QLJ7yAr_",
      "metadata": {
        "id": "_Zg5QLJ7yAr_"
      },
      "outputs": [],
      "source": [
        "answer_to_id = {ans: i for i, ans in enumerate(answer_space)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y__1xOMUxzsH",
      "metadata": {
        "id": "y__1xOMUxzsH"
      },
      "outputs": [],
      "source": [
        "missing_answers_train = train_df[~train_df[\"answer\"].isin(answer_to_id.keys())]\n",
        "missing_answers_test = test_df[~test_df[\"answer\"].isin(answer_to_id.keys())]\n",
        "\n",
        "print(f\"Missing in train: {len(missing_answers_train)}\")\n",
        "print(f\"Missing in test: {len(missing_answers_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DCv53YJhyKno",
      "metadata": {
        "id": "DCv53YJhyKno"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[train_df[\"answer\"].isin(answer_to_id.keys())].copy()\n",
        "test_df = test_df[test_df[\"answer\"].isin(answer_to_id.keys())].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lBbUMIT13ySq",
      "metadata": {
        "id": "lBbUMIT13ySq"
      },
      "outputs": [],
      "source": [
        "# Map answers to class indices\n",
        "# answer_to_id = {ans: i for i, ans in enumerate(answer_space)}\n",
        "train_df[\"label\"] = train_df[\"answer\"].map(answer_to_id)\n",
        "test_df[\"label\"] = test_df[\"answer\"].map(answer_to_id)\n",
        "\n",
        "# Ensure labels are integers and not NaN\n",
        "train_df = train_df.dropna(subset=[\"label\"]).copy()\n",
        "test_df = test_df.dropna(subset=[\"label\"]).copy()\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
        "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OV5qmFQ8pxyz",
      "metadata": {
        "id": "OV5qmFQ8pxyz"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hVFv0hJT_DJP",
      "metadata": {
        "id": "hVFv0hJT_DJP"
      },
      "source": [
        "Crate a Hugging Face Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ltfEKLTyukOM",
      "metadata": {
        "id": "ltfEKLTyukOM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fv1THsaN3yLq",
      "metadata": {
        "id": "fv1THsaN3yLq"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NlWBuqgb_NDn",
      "metadata": {
        "id": "NlWBuqgb_NDn"
      },
      "source": [
        "Preprocess function of ViLT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4jAbnQYr3js0",
      "metadata": {
        "id": "4jAbnQYr3js0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import ViltProcessor\n",
        "from PIL import Image\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "image_dir = os.path.join(base_dir, \"images\")\n",
        "\n",
        "def preprocess(example):\n",
        "    image_path = os.path.join(image_dir, f\"{example['image_id']}.png\")\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Use return_tensors=\"np\" to avoid PyTorch tensor overload\n",
        "    inputs = processor(\n",
        "        text=example[\"question\"],\n",
        "        images=image,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # Flatten tensors to remove batch dim\n",
        "    inputs = {k: v.squeeze(0) if hasattr(v, \"shape\") and v.shape[0] == 1 else v for k, v in inputs.items()}\n",
        "    inputs[\"labels\"] = int(example[\"labels\"])\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlUCLN1x4Y6R",
      "metadata": {
        "id": "hlUCLN1x4Y6R"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.rename_column(\"label\", \"labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PmyPyQQV_Vnc",
      "metadata": {
        "id": "PmyPyQQV_Vnc"
      },
      "outputs": [],
      "source": [
        "# Apply pre-processing\n",
        "#dataset = dataset.map(preprocess)\n",
        "dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JKfG-pP46Ejj",
      "metadata": {
        "id": "JKfG-pP46Ejj"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"train\"][0][\"labels\"], type(dataset[\"train\"][0][\"labels\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X3vHhTlVWwju",
      "metadata": {
        "id": "X3vHhTlVWwju"
      },
      "outputs": [],
      "source": [
        "small_dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"].select(range(100)),\n",
        "    \"test\": dataset[\"test\"].select(range(20)),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFC0kr0nkS2T",
      "metadata": {
        "id": "JFC0kr0nkS2T"
      },
      "outputs": [],
      "source": [
        "# Try 500 examples\n",
        "medium_dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"].select(range(500)),\n",
        "    \"test\": dataset[\"test\"].select(range(100)),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lRCWK_md_Vhk",
      "metadata": {
        "id": "lRCWK_md_Vhk"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "\n",
        "from transformers import ViltForQuestionAnswering\n",
        "import os  # <-- missing import\n",
        "\n",
        "# Define the base directory (your dataset folder in Google Drive)\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "\n",
        "# Load the list of all possible answers\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    answer_space = f.read().splitlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oCApeuR9I9Pg",
      "metadata": {
        "id": "oCApeuR9I9Pg"
      },
      "outputs": [],
      "source": [
        "model = ViltForSingleLabelQA.from_pretrained(\n",
        "    \"dandelin/vilt-b32-mlm\",\n",
        "    num_labels=len(answer_space)\n",
        ")\n",
        "model.config.problem_type = \"single_label_classification\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UfQmISV42AQQ",
      "metadata": {
        "id": "UfQmISV42AQQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode IDs into answer strings\n",
        "    pred_answers = [answer_space[p] for p in preds]\n",
        "    true_answers = [answer_space[l] for l in labels]\n",
        "\n",
        "    # WUPS across all pairs\n",
        "    wups = [\n",
        "        wup_measure(pred, true)\n",
        "        for pred, true in zip(pred_answers, true_answers)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"wups\": np.mean(wups)\n",
        "    }\n",
        "'''\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    preds = logits.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds)\n",
        "    }\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MMQkR0tQ2AN-",
      "metadata": {
        "id": "MMQkR0tQ2AN-"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vilt_daquar_output\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        "    #label_smoothing_factor=0.1,  # âœ… Try 0.1 to start\n",
        "    remove_unused_columns=False  # âœ… ADD THIS LINE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2-jiubn2ALm",
      "metadata": {
        "id": "j2-jiubn2ALm"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "def data_collator(features):\n",
        "    batch = default_data_collator(features)\n",
        "    batch[\"labels\"] = batch[\"labels\"].long()  # ðŸ’¥ force long tensor\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7nGvraZ62AJl",
      "metadata": {
        "id": "7nGvraZ62AJl"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "'''\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_dataset[\"train\"],\n",
        "    eval_dataset=small_dataset[\"test\"],\n",
        "    tokenizer=processor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "'''\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=processor,\n",
        "    data_collator=data_collator,  # <-- custom one\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqQJH668IrY-",
      "metadata": {
        "id": "cqQJH668IrY-"
      },
      "outputs": [],
      "source": [
        "model.config.problem_type = \"single_label_classification\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Vi9N9Ec2AHX",
      "metadata": {
        "id": "8Vi9N9Ec2AHX"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming `dataset` is your DatasetDict\n",
        "# print(dataset[\"train\"][0])  # Single example\n",
        "\n",
        "# Or view the first few rows\n",
        "for i in range(3):\n",
        "    print(dataset[\"train\"][i])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W_h5yPj0IKvK"
      },
      "id": "W_h5yPj0IKvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DUqP6pHi2AFN",
      "metadata": {
        "id": "DUqP6pHi2AFN"
      },
      "outputs": [],
      "source": [
        "# stop here,\n",
        "# need to scale gradually with slicing\n",
        "# preprocess and scale to disk\n",
        "# resize images... how to create demo photos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dSIv9PLa2ACm",
      "metadata": {
        "id": "dSIv9PLa2ACm"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_lBx9_1WuiX7",
      "metadata": {
        "id": "_lBx9_1WuiX7"
      },
      "outputs": [],
      "source": [
        "dataset.save_to_disk(\"/content/drive/MyDrive/FinalProject/vilt_preprocessed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wqKTO9yrurkD",
      "metadata": {
        "id": "wqKTO9yrurkD"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "with open(\"/content/drive/MyDrive/FinalProject/metrics.json\", \"w\") as f:\n",
        "    import json\n",
        "    json.dump(metrics, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RPpoMhkTwyx-",
      "metadata": {
        "id": "RPpoMhkTwyx-"
      },
      "outputs": [],
      "source": [
        "def print_parameter_count(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"ðŸ§  Total parameters:     {total:,}\")\n",
        "    print(f\"ðŸŽ¯ Trainable parameters: {trainable:,}\")\n",
        "    print(f\"ðŸª¶ Frozen parameters:    {total - trainable:,}\")\n",
        "\n",
        "print_parameter_count(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ewKtupvz1-MX",
      "metadata": {
        "id": "ewKtupvz1-MX"
      },
      "outputs": [],
      "source": [
        "# stop here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be95ZuxdYje",
      "metadata": {
        "id": "4be95ZuxdYje"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TWXhVIZLRB44",
      "metadata": {
        "id": "TWXhVIZLRB44"
      },
      "outputs": [],
      "source": [
        "# validated invered results\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def predict_vilt(image_path, question, model, processor, answer_space):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(image, question, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        pred_idx = outputs.logits.argmax(-1).item()\n",
        "        return answer_space[pred_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VTIPFO_tRTDK",
      "metadata": {
        "id": "VTIPFO_tRTDK"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "image_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image3.png\"\n",
        "question = \"What is under the cabinet near the sink?\"\n",
        "\n",
        "answer = predict_vilt(image_path, question, model, processor, answer_space)\n",
        "print(\"Predicted Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1sMB0VwRkM5",
      "metadata": {
        "id": "B1sMB0VwRkM5"
      },
      "outputs": [],
      "source": [
        "# evaluate performance\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc = accuracy_score(test_df[\"answer\"], test_df[\"vilt_pred\"])\n",
        "print(f\"ViLT Accuracy on Test Set: {acc:.3f}\")\n",
        "\n",
        "# Save to CSV\n",
        "# test_df.to_csv(os.path.join(base_dir, \"vilt_predictions.csv\"), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U6B77hz4RWqQ",
      "metadata": {
        "id": "U6B77hz4RWqQ"
      },
      "outputs": [],
      "source": [
        "# run Batch inference on Evaluate set\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "vilt_preds = []\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    image_path = os.path.join(base_dir, \"images\", f\"{row['image_id']}.png\")\n",
        "    question = row[\"question\"]\n",
        "    pred = predict_vilt(image_path, question, model, processor, answer_space)\n",
        "    vilt_preds.append(pred)\n",
        "\n",
        "test_df[\"vilt_pred\"] = vilt_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gG2ZbdjQ_Vel",
      "metadata": {
        "id": "gG2ZbdjQ_Vel"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTKrUpVF_Vbk",
      "metadata": {
        "id": "OTKrUpVF_Vbk"
      },
      "outputs": [],
      "source": [
        "# Replace NaN with a string\n",
        "df['answer'] = df['answer'].fillna('Missing')\n",
        "\n",
        "# Recalculate the counts\n",
        "answer_counts = df['answer'].value_counts()\n",
        "\n",
        "# Plot histogram of top N most common answers (e.g., top 50)\n",
        "top_n = 50\n",
        "plt.figure(figsize=(12, 6))\n",
        "answer_counts[:top_n].plot(kind='bar')\n",
        "plt.title(f'Top {top_n} Most Common Answers (including Missing)')\n",
        "plt.xlabel('Answer')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q36sZE0cdGxl",
      "metadata": {
        "id": "Q36sZE0cdGxl"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltForQuestionAnswering\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\n",
        "    \"dandelin/vilt-b32-mlm\",\n",
        "    num_labels=len(answer_space)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qd_jejTcdGu_",
      "metadata": {
        "id": "Qd_jejTcdGu_"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168O5PZ1dUr-",
      "metadata": {
        "id": "168O5PZ1dUr-"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "data_collator = DefaultDataCollator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tPoUA35Ndac1",
      "metadata": {
        "id": "tPoUA35Ndac1"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vilt_daquar_output\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",  #try eval_Str\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2EOYVtftpkx",
      "metadata": {
        "id": "c2EOYVtftpkx"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum(axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1W_3X2I_VYI",
      "metadata": {
        "id": "c1W_3X2I_VYI"
      },
      "outputs": [],
      "source": [
        "# Sample DataFrame with multiple columns\n",
        "data = {\n",
        "    'question': ['q1', 'q2', 'q3', 'q4', 'q5'],\n",
        "    'answer': ['yes', None, 'no', 'maybe', None],\n",
        "    'confidence': [0.9, 0.8, None, 0.7, 0.6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plot the number of missing values per column\n",
        "df.isna().sum().plot(kind='bar')\n",
        "plt.title('Number of Missing Values per Column')\n",
        "plt.xlabel('Column')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}