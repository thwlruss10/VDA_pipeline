{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlruss10/VDA_pipeline/blob/main/GIT_DEMO_VQA_Bert_DEiT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-CQpvTZ_vsSN",
      "metadata": {
        "id": "-CQpvTZ_vsSN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zioGKih-t97P",
      "metadata": {
        "id": "zioGKih-t97P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd062d49",
      "metadata": {
        "id": "dd062d49"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models & transformers (ViTModel, DeiTModel, BEiT)\n",
        "    AutoModel,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "# SET ONLY 1 GPU DEVICE\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "#set_caching_enabled(True)>> this line deleted as 'datasets' no longer supports explicit cahce enabling\n",
        "#set_caching_enabled(True)\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above code imports dependencies, sets cache path for Huggingface transformers, error logs message threshold set to supress warnings deemed not severe. GPU device set for torch processes."
      ],
      "metadata": {
        "id": "gyxWJWH2z6Xc"
      },
      "id": "gyxWJWH2z6Xc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TF7cDaBPP06j",
      "metadata": {
        "id": "TF7cDaBPP06j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory pointing to your Drive location\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Define regex to extract image ID\n",
        "image_pattern = re.compile(r\"( (in |on |of )?(the |this )?(image\\d*) \\?)\")\n",
        "\n",
        "# Read the raw Q&A file\n",
        "# Extracts image ID and question at index[i] together with answer at [i+1]\n",
        "# and organizes it into a pandas DataFrame, creates a list of unique answers,\n",
        "# and then splits the dataset into training and testing sets, saving them as\n",
        "# CSV.\n",
        "\n",
        "qa_file = os.path.join(base_dir, \"all_qa_pairs.txt\")\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as f: # open in read mode with in utf-8 encoding\n",
        "    qa_data = [x.strip() for x in f.readlines()] # x.strip() removes white spaces\n",
        "\n",
        "records = []\n",
        "for i in range(0, len(qa_data), 2): # iterates in steps of two because quesiton is at index i and answer at i+1\n",
        "    match = image_pattern.findall(qa_data[i]) # extract image ID from question string\n",
        "    if match:\n",
        "        img_id = match[0][3]\n",
        "        question = qa_data[i].replace(match[0][0], \"\").strip() # remove image ID and replace question\n",
        "        answer = qa_data[i + 1].strip() # answer retrieved from next element\n",
        "        records.append({\"question\": question, \"answer\": answer, \"image_id\": img_id}) #populate dictionary\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "\n",
        "answer_space = []\n",
        "for ans in df[\"answer\"].to_list():\n",
        "    if \",\" in ans:\n",
        "        answer_space += ans.replace(\" \", \"\").split(\",\")\n",
        "    else:\n",
        "        answer_space.append(ans)\n",
        "\n",
        "# sort and remove duplicates from answer_space\n",
        "answer_space = sorted(set(answer_space))\n",
        "\n",
        "# Write the answer space to file\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(\"\\n\".join(answer_space))\n",
        "\n",
        "# Split dataset into training and evaluation\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save to CSV\n",
        "train_df.to_csv(os.path.join(base_dir, \"data_train.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(base_dir, \"data_eval.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "eval_df = pd.read_csv(os.path.join(base_dir, \"data_eval.csv\"))\n"
      ],
      "metadata": {
        "id": "j9nPozrCG5_O"
      },
      "id": "j9nPozrCG5_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VeFvmlWWQN-l",
      "metadata": {
        "id": "VeFvmlWWQN-l"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Define the base directory (your dataset folder in Google Drive)\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "\n",
        "# Load the training & evaluation dataset from CSV files\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\n",
        "        \"train\": os.path.join(base_dir, \"data_train.csv\"),\n",
        "\n",
        "\n",
        "\n",
        "        \"test\": os.path.join(base_dir, \"data_eval.csv\")\n",
        "    }\n",
        ")\n",
        "\n",
        "# Load the list of all possible answers\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    answer_space = f.read().splitlines()\n",
        "\n",
        "# Map textual answers to label indices (for classification)\n",
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        'label': [\n",
        "            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0])\n",
        "            for ans in examples['answer']\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pPbNmR7xQgVW",
      "metadata": {
        "id": "pPbNmR7xQgVW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set the correct image directory in Google Drive\n",
        "image_dir = \"/content/drive/MyDrive/FinalProject/dataset/images\"\n",
        "\n",
        "def showExample(train=True, id=None):\n",
        "    if train:\n",
        "        data = dataset[\"train\"]\n",
        "    else:\n",
        "        data = dataset[\"test\"]\n",
        "\n",
        "    if id is None:\n",
        "        id = np.random.randint(len(data))\n",
        "\n",
        "    img_path = os.path.join(image_dir, data[id][\"image_id\"] + \".png\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"⚠️ Image not found at: {img_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"✅ Found image at: {os.path.abspath(img_path)}\")\n",
        "    image = Image.open(img_path)\n",
        "    display(image)\n",
        "\n",
        "    print(\"Question:\\t\", data[id][\"question\"])\n",
        "    print(\"Answer:\\t\\t\", data[id][\"answer\"], f\"(Label: {data[id]['label']})\")\n",
        "\n",
        "# Show a random training example\n",
        "showExample()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From project prompt:\n",
        " EXPLORE ATTENTNION MECHANISMS AND CROSS-MODAL FEATURE FUSION TECHINQUES"
      ],
      "metadata": {
        "id": "CocWSo08Jw7P"
      },
      "id": "CocWSo08Jw7P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bigl0Nh2RU0Q",
      "metadata": {
        "id": "bigl0Nh2RU0Q"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Union\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "'''\n",
        "Defines class called mulitmodal collator which encodes/tokenizes text and images.\n",
        "advance of transformers\n",
        "\n",
        "__call__ is when the MultimodalCollator is used to prepare a batch of data.\n",
        "It takes a raw_batch_dict as input, each containing information about a single data point.\n",
        "It extracts the question, image_id, and label (answer) from the dictionary.\n",
        "It then calls tokenize_text and preprocess_images to process the text and images,respectively.\n",
        "Finally, it combines the results into a single dictionary containing the tokenized text,\n",
        "processed images, and labels, ready to be fed to the model.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MultimodalCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    preprocessor: AutoFeatureExtractor\n",
        "    image_dir: str  # ✅ add image_dir as a configurable path\n",
        "\n",
        "    def tokenize_text(self, texts: List[str]):\n",
        "        encoded_text = self.tokenizer(\n",
        "            text=texts,\n",
        "            padding='longest',\n",
        "            max_length=24,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_text['input_ids'],\n",
        "            \"token_type_ids\": encoded_text['token_type_ids'],\n",
        "            \"attention_mask\": encoded_text['attention_mask'],\n",
        "        }\n",
        "\n",
        "    def preprocess_images(self, images: List[str]):\n",
        "        # ✅ Read from provided image_dir\n",
        "        processed_images = self.preprocessor(\n",
        "            images=[\n",
        "                Image.open(os.path.join(self.image_dir, image_id + \".png\")).convert('RGB')\n",
        "                for image_id in images\n",
        "            ],\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"pixel_values\": processed_images['pixel_values'],\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, raw_batch_dict: Union[dict, List[dict]]):\n",
        "        # Normalize format to list of dicts\n",
        "        if isinstance(raw_batch_dict, dict):\n",
        "            questions = raw_batch_dict['question']\n",
        "            image_ids = raw_batch_dict['image_id']\n",
        "            labels = raw_batch_dict['label']\n",
        "        else:\n",
        "            questions = [i['question'] for i in raw_batch_dict]\n",
        "            image_ids = [i['image_id'] for i in raw_batch_dict]\n",
        "            labels = [i['label'] for i in raw_batch_dict]\n",
        "\n",
        "        return {\n",
        "            **self.tokenize_text(questions),\n",
        "            **self.preprocess_images(image_ids),\n",
        "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m0T3xWzzRyfq",
      "metadata": {
        "id": "m0T3xWzzRyfq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from typing import Optional\n",
        "\n",
        "'''\n",
        "The MultimodalVQAModel takes a question and an image as input.\n",
        "It uses pre-trained models to extract features from both, combines these features,\n",
        "and then predicts the answer to the question based on the combined information.\n",
        "This architecture is typical for VQA tasks, where the goal is to understand both\n",
        "visual and textual information to provide accurate answers.\n",
        "\n",
        "forward() Defines how the model processes data during training and inference.\n",
        "  input_ids: Tokenized representation of the question.\n",
        "  pixel_values: Processed image data.\n",
        "  attention_mask: Indicates which tokens in the question are relevant.\n",
        "  token_type_ids: Distinguishes between different segments of text (if applicable).\n",
        "  labels: The correct answer (used during training).\n",
        "'''\n",
        "\n",
        "class MultimodalVQAModel(nn.Module):  #Defining Mu-Mo-VQA model is per PyTorch base class nn.Module\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_text_name: str,\n",
        "        pretrained_image_name: str,\n",
        "        num_labels: int,\n",
        "        intermediate_dim: int = 512,  # increase to reduce overfitting can modify to 1024\n",
        "        dropout: float = 0.5          # prevents overfitting, can decrease to 0.1.\n",
        "    ):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        self.num_labels = num_labels  #length of answer space\n",
        "        self.text_encoder = AutoModel.from_pretrained(pretrained_text_name) # attention mechanism lives in pre-trained transformers\n",
        "        self.image_encoder = AutoModel.from_pretrained(pretrained_image_name)\n",
        "\n",
        "        # Fallback if pooler_output doesn't exist (e.g., in ViT)\n",
        "        self.use_pooler_text = hasattr(self.text_encoder.config, \"pooler_fc_size\")\n",
        "        self.use_pooler_image = hasattr(self.image_encoder.config, \"pooler_fc_size\")\n",
        "\n",
        "        # Fusion + Classification layers\n",
        "        # linear transformation, activation function (ReLU), dropout regularization\n",
        "        # Integrates the combined features comprehensive representation\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size,\n",
        "                intermediate_dim\n",
        "            ),\n",
        "            # nn.BatchNorm1d(intermediate_dim),  # Add Batch Normalization\n",
        "            nn.ReLU(),            # alt nn.LeakyReLU, nn.GELU, or nn.ELU\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    # Feed forward fusion\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None\n",
        "    ):\n",
        "        # Encode text\n",
        "        text_output = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        if \"pooler_output\" in text_output:          # not all pre-trained models have pooling, BERT has\n",
        "            text_feat = text_output.pooler_output   # model to handle different pre-trained encoders by checking\n",
        "        else:\n",
        "            text_feat = text_output.last_hidden_state[:, 0]  # CLS token fallback\n",
        "\n",
        "        # Encode image\n",
        "        image_output = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        if \"pooler_output\" in image_output:\n",
        "            image_feat = image_output.pooler_output\n",
        "        else:\n",
        "            image_feat = image_output.last_hidden_state[:, 0]  # CLS token fallback\n",
        "\n",
        "        # Fuse + classify\n",
        "        fused = self.fusion(torch.cat([text_feat, image_feat], dim=1))\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        output = {\"logits\": logits}\n",
        "        if labels is not None:\n",
        "            output[\"loss\"] = self.criterion(logits, labels)\n",
        "\n",
        "        return output # Returns logit values associated with each label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9TnodqFTiXX"
      },
      "id": "n9TnodqFTiXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F-hf2KpwR7vL",
      "metadata": {
        "id": "F-hf2KpwR7vL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoImageProcessor  # use ImageProcessor (not deprecated)\n",
        "import torch\n",
        "\n",
        "'''\n",
        "set up components for VQA\n",
        "  MultiModal_collator: prepares input\n",
        "  multimodal_model: defines model architecture\n",
        "\n",
        "tokenizer: An object from the transformers library that is used to convert\n",
        "  text (questions) into numerical representations that the text model (bert-base-uncased in\n",
        "  this case) can understand.\n",
        "\n",
        "preprocessor: This object is also from transformers and handles the preprocessing of\n",
        "  images before they are fed to the image model (facebook/deit-base-patch16-224).\n",
        "\n",
        "It creates an instance of your MultimodalVQAModel class. This is the core of your VQA system.\n",
        "It takes: The names of the pre-trained text and image models & the number of possible answers.\n",
        "Also ensures that the model is placed on the chosen computing device (CPU or GPU).\n",
        "\n",
        "'''\n",
        "\n",
        "def createMultimodalVQACollatorAndModel(\n",
        "    text=\"bert-base-uncased\",\n",
        "    image=\"facebook/deit-base-patch16-224\",\n",
        "    image_dir=\"/content/drive/MyDrive/FinalProject/dataset/images\",\n",
        "    answer_space=None,  # pass in answer_space list here\n",
        "    device=None\n",
        "):\n",
        "    if answer_space is None:\n",
        "        raise ValueError(\"answer_space must be provided to set the number of classification labels.\")\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize tokenizer and image processor\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
        "    preprocessor = AutoImageProcessor.from_pretrained(image)\n",
        "\n",
        "    # Create collator with image_dir\n",
        "    multimodal_collator = MultimodalCollator(\n",
        "        tokenizer=tokenizer,\n",
        "        preprocessor=preprocessor,\n",
        "        image_dir=image_dir\n",
        "    )\n",
        "\n",
        "    # Initialize model with number of labels\n",
        "    multimodal_model = MultimodalVQAModel(\n",
        "        pretrained_text_name=text,\n",
        "        pretrained_image_name=image,\n",
        "        num_labels=len(answer_space)\n",
        "    ).to(device)\n",
        "\n",
        "    return multimodal_collator, multimodal_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "\n",
        "'''\n",
        "This code defines a function called wup_measure which calculates the semantic similarity\n",
        "between two words (a and b) using the Wu-Palmer similarity measure.\n",
        "This measure is based on the WordNet lexical database.\n",
        "\n",
        "\n",
        " This code uses the WordNet database to find the semantic relationship between two\n",
        " words and quantifies their similarity based on their position in the WordNet hierarchy.\n",
        "\n",
        "\n",
        "# Define wup_measure Function: This function takes two words (a, b)\n",
        "# and an optional similarity_threshold as input.\n",
        "# This function uses wn.synsets to get all the synonym sets (synsets) for the\n",
        "# word a, considering only nouns (pos=wn.NOUN).\n",
        "# It returns the synsets and a weight (currently set to 1.0).\n",
        "\n",
        "'''\n",
        "\n",
        "def wup_measure(a, b, similarity_threshold=0.925):\n",
        "    def get_semantic_field(a):\n",
        "        weight = 1.0\n",
        "        semantic_field = wn.synsets(a, pos=wn.NOUN)   # nets all synonym sets for word a\n",
        "        return (semantic_field, weight)\n",
        "\n",
        "    def get_stem_word(a):\n",
        "        weight = 1.0\n",
        "        return (a, weight)\n",
        "\n",
        "    global_weight = 1.0\n",
        "    (a, global_weight_a) = get_stem_word(a)\n",
        "    (b, global_weight_b) = get_stem_word(b)\n",
        "    global_weight = min(global_weight_a, global_weight_b)\n",
        "\n",
        "    if a == b:\n",
        "        return 1.0 * global_weight\n",
        "    if a == [] or b == []:\n",
        "        return 0\n",
        "\n",
        "    interp_a, weight_a = get_semantic_field(a)\n",
        "    interp_b, weight_b = get_semantic_field(b)\n",
        "\n",
        "    if interp_a == [] or interp_b == []:\n",
        "        return 0\n",
        "\n",
        "  # Calculate Similarity: Initial Checks: If the words are identical (a == b),\n",
        "  # it returns 1.0 (maximum similarity). If either word is empty, it returns 0.\n",
        "  # Get Semantic Fields: Calls get_semantic_field to retrieve the synsets for both words.\n",
        "\n",
        "  # Iterate and Compare: It iterates through all synset pairs for the two words\n",
        "  # and calculates their Wu-Palmer similarity using x.wup_similarity(y).\n",
        "  # It stores the maximum similarity found in global_max.\n",
        "    global_max = 0.0\n",
        "    for x in interp_a:\n",
        "        for y in interp_b:\n",
        "            local_score = x.wup_similarity(y)\n",
        "            if local_score and local_score > global_max:\n",
        "                global_max = local_score\n",
        "  # Apply Threshold: If the maximum similarity is above the similarity_threshold,\n",
        "  # it assigns a weight of 1.0; otherwise, it assigns a lower weight of 0.1.\n",
        "    interp_weight = 1.0 if global_max >= similarity_threshold else 0.1\n",
        "    #Calculates the final similarity score by multiplying the maximum similarity with various weights.\n",
        "    final_score = global_max * weight_a * weight_b * interp_weight * global_weight\n",
        "    return final_score  # returns the calculated Wu-Palmer similarity score between the two words.\n"
      ],
      "metadata": {
        "id": "rMvIal8Ff56F"
      },
      "id": "rMvIal8Ff56F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Dict\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "'''\n",
        "    The function returns a dictionary containing the calculated metrics\n",
        "    \"wups\": The average Wu-Palmer similarity score.\n",
        "    \"acc\": The accuracy.\n",
        "    \"f1\": The macro-averaged F1 score.\n",
        "\n",
        "    eval_preds: This is the output from the model's evaluation step. It contains:\n",
        "    logits: Raw predictions from the model.\n",
        "    labels: The true answers for the evaluation data.\n",
        "    answer_space: A list containing all possible answers.\n",
        "'''\n",
        "def compute_metrics(eval_preds, answer_space) -> Dict[str, float]:\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    def batch_wup_measure(labels, preds):\n",
        "        return np.mean([\n",
        "            wup_measure(answer_space[label], answer_space[pred])\n",
        "            for label, pred in zip(labels, preds)\n",
        "        ])\n",
        "\n",
        "    return {\n",
        "        \"wups\": batch_wup_measure(labels, preds),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro')\n",
        "    }\n"
      ],
      "metadata": {
        "id": "PXvMLajagD8H"
      },
      "id": "PXvMLajagD8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "compute_metrics_with_answer_space = partial(compute_metrics, answer_space=answer_space)\n"
      ],
      "metadata": {
        "id": "VEV-dWaigpNN"
      },
      "id": "VEV-dWaigpNN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1IbJv5c-Sj7n",
      "metadata": {
        "id": "1IbJv5c-Sj7n"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "'''\n",
        "code snippet focuses on setting up and configuring the training process for\n",
        "a multimodal Visual Question Answering (VQA) model\n",
        "\n",
        "  It defines the model, configures training parameters, prepares the data,\n",
        "  and uses the Hugging Face Trainer to manage the training process.\n",
        "  The commented-out section provides an option for subsampling the data during development.\n",
        "  It imports necessary modules from the transformers library: TrainingArguments\n",
        "  for configuring the training process and Trainer for managing the training loop.\n",
        "  torch is imported for PyTorch operations.\n",
        "  device is set to use a GPU if available (cuda), otherwise it defaults to the CPU.\n",
        "\n",
        "'''\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image directory and answer space from earlier\n",
        "image_dir = \"/content/drive/MyDrive/FinalProject/dataset/images\"\n",
        "\n",
        "# Initialize collator and model\n",
        "collator, model = createMultimodalVQACollatorAndModel(\n",
        "    text=\"bert-base-uncased\",\n",
        "    image=\"facebook/deit-base-patch16-224\",\n",
        "    image_dir=image_dir,\n",
        "    answer_space=answer_space,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "multi_args = TrainingArguments(\n",
        "    output_dir=\"checkpoint\",\n",
        "    run_name=\"multimodal-vqa-run\",\n",
        "    seed=12345,\n",
        "    eval_strategy=\"steps\",             # ✅ updated from deprecated\n",
        "    eval_steps=100,                     # how often to evaluate the model during training\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,                  # how often to log training progress\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,                       # How often to save model checkpoints\n",
        "    save_total_limit=3,                 # max no. checkpoints to keep\n",
        "    metric_for_best_model='wups',       # Wu-Palmer similarity\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=5, # no. of training epochs resuced to expedite debugging\n",
        "    #num_train_epochs=5,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,          # ✅ better for Colab\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Wrap compute_metrics so it includes answer_space\n",
        "def compute_metrics_with_answer_space(eval_preds):\n",
        "    return compute_metrics(eval_preds, answer_space=answer_space)\n",
        "\n",
        "# Initialize Trainer\n",
        "\n",
        "multi_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=multi_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics_with_answer_space\n",
        ")\n",
        "'''\n",
        "This commented-out section suggests a way to subsample the dataset for faster debugging and development.\n",
        "It selects a smaller portion of the training and evaluation datasets using dataset.select.\n",
        "\n",
        "# Subsample just 500 examples for dev training\n",
        "# Added to expedite training iterations and debugging\n",
        "# small_train = dataset['train'].select(range(1000))\n",
        "# small_eval = dataset['test'].select(range(100))\n",
        "\n",
        "\n",
        "# Initialize for Subsampled training data\n",
        "multi_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=multi_args,\n",
        "    train_dataset=small_train,\n",
        "    eval_dataset=small_eval,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics_with_answer_space\n",
        ")\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EaeM0ZHRYqNQ"
      },
      "id": "EaeM0ZHRYqNQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "train_multi_metrics = multi_trainer.train()\n",
        "\n",
        "# Final evaluation\n",
        "eval_multi_metrics = multi_trainer.evaluate()"
      ],
      "metadata": {
        "id": "huIHk6t0YgOb"
      },
      "id": "huIHk6t0YgOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IdTVo5Nt8C0q"
      },
      "id": "IdTVo5Nt8C0q"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/MM-VQA-demo.pt\")\n"
      ],
      "metadata": {
        "id": "QRdKHcIoAJjW"
      },
      "id": "QRdKHcIoAJjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh \"/content/drive/MyDrive/FinalProject/myenv/\"\n"
      ],
      "metadata": {
        "id": "DEg1ze05CGxb"
      },
      "id": "DEg1ze05CGxb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])"
      ],
      "metadata": {
        "id": "lONOjFRgIHTI"
      },
      "id": "lONOjFRgIHTI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "MirG_pvxpT1m"
      },
      "id": "MirG_pvxpT1m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/MM-VQA-demo.pt\")\n"
      ],
      "metadata": {
        "id": "Jt5v1Ahs_OT7"
      },
      "id": "Jt5v1Ahs_OT7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT.pt\")\n"
      ],
      "metadata": {
        "id": "QU4SRIodvokU"
      },
      "id": "QU4SRIodvokU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"pretrained_text_name\": \"bert-base-uncased\",\n",
        "    \"pretrained_image_name\": \"facebook/deit-base-patch16-224\",\n",
        "    \"num_labels\": len(answer_space),\n",
        "    \"intermediate_dim\": 512,\n",
        "    \"dropout\": 0.5\n",
        "}\n"
      ],
      "metadata": {
        "id": "lTcXSj-Mw_V8"
      },
      "id": "lTcXSj-Mw_V8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/VQA_Demo/model.pt\")\n",
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT.pt\")\n",
        "torch.save(config, \"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT_config.pt\")\n",
        "# Save the model\n"
      ],
      "metadata": {
        "id": "xwggs7C2wRYT"
      },
      "id": "xwggs7C2wRYT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT.pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m8gczA494cwc"
      },
      "id": "m8gczA494cwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT.pt\")\n"
      ],
      "metadata": {
        "id": "Hszl38Cg2FUC"
      },
      "id": "Hszl38Cg2FUC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors\n",
        "\n"
      ],
      "metadata": {
        "id": "h9iDHjTEDajd"
      },
      "id": "h9iDHjTEDajd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z3MN-z8wa1Km"
      },
      "id": "Z3MN-z8wa1Km"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/checkpoint"
      ],
      "metadata": {
        "id": "cuPGIWNBmu4g"
      },
      "id": "cuPGIWNBmu4g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use any of the saved model checkpoints for QA dialogue.\n",
        "The question must be tokenized, and image features must be extracted appropriately (as done in the collator). These would serve as input to the model, with weights loaded from the trained checkpoint. The label predicted by the model is then mapped to the index of the actual answer in the answer space."
      ],
      "metadata": {
        "id": "H3ou_vnxa18g"
      },
      "id": "H3ou_vnxa18g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "csR5q77mgCzR",
      "metadata": {
        "id": "csR5q77mgCzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from typing import List, Dict\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "'''\n",
        "def loadAnswerSpace() -> List[str]:\n",
        "    path = os.path.join(\"dataset\", \"answer_space.txt\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Answer space file not found at: {path}\")\n",
        "    with open(path) as f:\n",
        "        answer_space = f.read().splitlines()\n",
        "    return answer_space\n",
        "'''\n",
        "def loadAnswerSpace() -> List[str]:\n",
        "    base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "    path = os.path.join(base_dir, \"answer_space.txt\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Answer space file not found at: {path}\")\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        answer_space = f.read().splitlines()\n",
        "    return answer_space\n",
        "\n",
        "def tokenizeQuestion(text_encoder, question, device) -> Dict:\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(text_encoder)\n",
        "    encoded_text = tokenizer(\n",
        "        text=[question],\n",
        "        padding='longest',\n",
        "        max_length=24,\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": encoded_text['input_ids'].to(device),\n",
        "        \"token_type_ids\": encoded_text['token_type_ids'].to(device),\n",
        "        \"attention_mask\": encoded_text['attention_mask'].to(device),\n",
        "    }\n",
        "\n",
        "def featurizeImage(image_encoder, img_path, device) -> Dict:\n",
        "    featurizer = transformers.AutoFeatureExtractor.from_pretrained(image_encoder)\n",
        "    processed_images = featurizer(\n",
        "        images=[Image.open(img_path).convert('RGB')],\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return {\n",
        "        \"pixel_values\": processed_images['pixel_values'].to(device),\n",
        "    }\n",
        "\n",
        "# Example usage (make sure these variables are defined in your script)\n",
        "question = \"What is the color of the chair in front of the white wall?\"\n",
        "img_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image3.png\"\n",
        "# img_path = \"dataset/images/image100.png\"\n",
        "\n",
        "# Load the vocabulary of all answers\n",
        "answer_space = loadAnswerSpace()\n",
        "\n",
        "# Tokenize the question & featurize the image\n",
        "question = question.lower().replace(\"?\", \"\").strip()\n",
        "tokenized_question = tokenizeQuestion(\"bert-base-uncased\", question, device)\n",
        "featurized_img = featurizeImage(\"google/vit-base-patch16-224-in21k\", img_path, device)\n",
        "\n",
        "# Load the model checkpoint (example)\n",
        "model = MultimodalVQAModel(\n",
        "    pretrained_text_name=\"bert-base-uncased\",\n",
        "    pretrained_image_name=\"facebook/deit-base-patch16-224\",\n",
        "    num_labels=len(answer_space),\n",
        "    intermediate_dim=512\n",
        ")\n",
        "\n",
        "#checkpoint = \"/content/drive/MyDrive/FinalProject/checkpoint/checkpoint-1500/pytorch_model.bin\"\n",
        "checkpoint_path = \"/content/checkpoint/checkpoint-1560/model.safetensors\"\n",
        "#model.load_state_dict(torch.load(checkpoint, map_location=device))\n",
        "#checkpoint = os.path.join(\"checkpoint\", \"checkpoint-1500\", \"pytorch_model.bin\")\n",
        "#model.load_state_dict(torch.load(checkpoint))\n",
        "model.load_state_dict(load_file(checkpoint_path, device=str(device)))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Get the model's prediction\n",
        "input_ids = tokenized_question[\"input_ids\"]\n",
        "token_type_ids = tokenized_question[\"token_type_ids\"]\n",
        "attention_mask = tokenized_question[\"attention_mask\"]\n",
        "pixel_values = featurized_img[\"pixel_values\"]\n",
        "\n",
        "output = model(input_ids, pixel_values, attention_mask, token_type_ids)\n",
        "\n",
        "# Map prediction to answer\n",
        "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
        "answer = answer_space[preds[0]]\n",
        "print(\"Predicted Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u88qCqaxRgF9",
      "metadata": {
        "id": "u88qCqaxRgF9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}