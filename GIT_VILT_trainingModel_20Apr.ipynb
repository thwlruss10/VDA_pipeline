{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlruss10/VDA_pipeline/blob/main/GIT_VILT_trainingModel_20Apr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.1 accelerate==0.21.0 peft==0.4.0"
      ],
      "metadata": {
        "id": "mcvwLIlSDv8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6i6uVho_318j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zioGKih-t97P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.28.1 datasets nltk scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "gl_sT9fc369R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models & transformers (ViTModel, DeiTModel, BEiT)\n",
        "    AutoModel,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "# SET ONLY 1 GPU DEVICE\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "#set_caching_enabled(True)>> this line deleted as 'datasets' no longer supports explicit cahce enabling\n",
        "#set_caching_enabled(True)\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "kr2XKP2d4Emp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory pointing to your Drive location\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Define regex to extract image ID\n",
        "image_pattern = re.compile(r\"( (in |on |of )?(the |this )?(image\\d*) \\?)\")\n",
        "\n",
        "# Read the raw Q&A file\n",
        "# Extracts image ID and question at index[i] together with answer at [i+1]\n",
        "# and organizes it into a pandas DataFrame, creates a list of unique answers,\n",
        "# and then splits the dataset into training and testing sets, saving them as\n",
        "# CSV.\n",
        "\n",
        "qa_file = os.path.join(base_dir, \"all_qa_pairs.txt\")\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as f: # open in read mode with in utf-8 encoding\n",
        "    qa_data = [x.strip() for x in f.readlines()] # x.strip() removes white spaces\n",
        "\n",
        "records = []\n",
        "for i in range(0, len(qa_data), 2): # iterates in steps of two because quesiton is at index i and answer at i+1\n",
        "    match = image_pattern.findall(qa_data[i]) # extract image ID from question string\n",
        "    if match:\n",
        "        img_id = match[0][3]\n",
        "        question = qa_data[i].replace(match[0][0], \"\").strip() # remove image ID and replace question\n",
        "        answer = qa_data[i + 1].strip() # answer retrieved from next element\n",
        "        records.append({\"question\": question, \"answer\": answer, \"image_id\": img_id}) #populate dictionary\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "\n",
        "answer_space = []\n",
        "for ans in df[\"answer\"].to_list():\n",
        "    if \",\" in ans:\n",
        "        answer_space += ans.replace(\" \", \"\").split(\",\")\n",
        "    else:\n",
        "        answer_space.append(ans)\n",
        "\n",
        "# sort and remove duplicates from answer_space\n",
        "answer_space = sorted(set(answer_space))\n",
        "\n",
        "# Write the answer space to file\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(\"\\n\".join(answer_space))\n",
        "\n",
        "# Split dataset into training and evaluation\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save to CSV\n",
        "train_df.to_csv(os.path.join(base_dir, \"data_train.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(base_dir, \"data_eval.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "sUMJUc3K4KbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "eval_df = pd.read_csv(os.path.join(base_dir, \"data_eval.csv\"))"
      ],
      "metadata": {
        "id": "QAwGOeVZ4TmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAIlZDbO4Td3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers.models.vilt.modeling_vilt import ViltForQuestionAnswering\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ViltForSingleLabelQA(ViltForQuestionAnswering):\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        pixel_values=None,\n",
        "        pixel_mask=None,\n",
        "        labels=None,\n",
        "        return_dict=True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # ðŸ§¼ Remove any Trainer-only args\n",
        "        kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "                # âœ… Manually generate position_ids to match input length\n",
        "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        # âœ… Forward to base ViLT model without passing labels (so we control loss)\n",
        "        outputs = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            pixel_mask=pixel_mask,\n",
        "            return_dict=return_dict,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # âœ… Our own classification loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if labels.dtype != torch.long:\n",
        "                labels = labels.long()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "'''"
      ],
      "metadata": {
        "id": "RJjIq1OT4TiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_dataset[\"train\"][0].keys())"
      ],
      "metadata": {
        "id": "1quNdlj3S8CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViltForSingleLabelQA(ViltForQuestionAnswering):\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        pixel_values=None,\n",
        "        pixel_mask=None,\n",
        "        labels=None,\n",
        "        return_dict=True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        outputs = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            pixel_mask=pixel_mask,\n",
        "            return_dict=return_dict,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if labels.dtype != torch.long:\n",
        "                labels = labels.long()\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "HxHblaElqvXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wup_measure(pred, gt, threshold=0.9):\n",
        "    pred_synsets = wn.synsets(pred)\n",
        "    gt_synsets = wn.synsets(gt)\n",
        "\n",
        "    if not pred_synsets or not gt_synsets:\n",
        "        return 0.0\n",
        "\n",
        "    max_score = max((s1.wup_similarity(s2) or 0) for s1 in pred_synsets for s2 in gt_synsets)\n",
        "    return 1.0 if max_score >= threshold else max_score\n"
      ],
      "metadata": {
        "id": "8yhWwnx84TYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load model and processor\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "'''\n",
        "# Load image and define question\n",
        "image_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image2.png\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "question = \"What is on the left side of the sink?\"\n",
        "\n",
        "# Preprocess inputs\n",
        "inputs = processor(image, question, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_answer = model.config.id2label[logits.argmax(-1).item()]\n",
        "\n",
        "print(\"Predicted Answer:\", predicted_answer)\n",
        "'''"
      ],
      "metadata": {
        "id": "Y9J-_tCQ4TVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_to_id = {ans: i for i, ans in enumerate(answer_space)}\n",
        "\n",
        "\n",
        "missing_answers_train = train_df[~train_df[\"answer\"].isin(answer_to_id.keys())]\n",
        "missing_answers_test = test_df[~test_df[\"answer\"].isin(answer_to_id.keys())]\n",
        "\n",
        "print(f\"Missing in train: {len(missing_answers_train)}\")\n",
        "print(f\"Missing in test: {len(missing_answers_test)}\")\n"
      ],
      "metadata": {
        "id": "eEm1Uh4-4TSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[train_df[\"answer\"].isin(answer_to_id.keys())].copy()\n",
        "test_df = test_df[test_df[\"answer\"].isin(answer_to_id.keys())].copy()"
      ],
      "metadata": {
        "id": "y0CqNy994TOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map answers to class indices\n",
        "# answer_to_id = {ans: i for i, ans in enumerate(answer_space)}\n",
        "train_df[\"label\"] = train_df[\"answer\"].map(answer_to_id)\n",
        "test_df[\"label\"] = test_df[\"answer\"].map(answer_to_id)\n",
        "\n",
        "# Ensure labels are integers and not NaN\n",
        "train_df = train_df.dropna(subset=[\"label\"]).copy()\n",
        "test_df = test_df.dropna(subset=[\"label\"]).copy()\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
        "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "fckcOIH84TJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "\n",
        "\n",
        "raw_dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n"
      ],
      "metadata": {
        "id": "6mPFoMia43qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "raw_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"test\": Dataset.from_pandas(test_df)\n",
        "})\n"
      ],
      "metadata": {
        "id": "cvR0eDM8NTZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = raw_dataset.remove_columns([\"__index_level_0__\"])\n"
      ],
      "metadata": {
        "id": "vmZMFm79XdLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_dataset[\"train\"].column_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "RhGFRdaLM1ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltProcessor\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "image_dir = os.path.join(base_dir, \"images\")\n",
        "\n",
        "def preprocess(example):\n",
        "    image_path = os.path.join(image_dir, f\"{example['image_id']}.png\")\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(\n",
        "        text=example[\"question\"],\n",
        "        images=image,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=384,\n",
        "    )\n",
        "\n",
        "    inputs = {\n",
        "        k: v.squeeze(0) if hasattr(v, \"shape\") and v.shape[0] == 1 else v\n",
        "        for k, v in inputs.items()\n",
        "    }\n",
        "\n",
        "    inputs[\"labels\"] = int(example[\"label\"])\n",
        "    return inputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxHuIBF4Lq_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = raw_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=raw_dataset[\"train\"].column_names  # removes old 'question', 'label', etc.\n",
        ")\n"
      ],
      "metadata": {
        "id": "XRe2xKftQ236"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import ViltProcessor\n",
        "from PIL import Image\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "image_dir = os.path.join(base_dir, \"images\")\n",
        "\n",
        "def preprocess(example):\n",
        "    image_path = os.path.join(image_dir, f\"{example['image_id']}.png\")\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Use return_tensors=\"np\" to avoid PyTorch tensor overload\n",
        "\n",
        "    inputs = processor(\n",
        "        text=example[\"question\"],\n",
        "        images=image,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    inputs = processor(\n",
        "    text=example[\"question\"],\n",
        "    images=image,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=384,              # âœ… Explicitly limit to ViLTâ€™s expected size\n",
        "    )\n",
        "\n",
        "    # Flatten tensors to remove batch dim\n",
        "    inputs = {k: v.squeeze(0) if hasattr(v, \"shape\") and v.shape[0] == 1 else v for k, v in inputs.items()}\n",
        "    inputs[\"labels\"] = int(example[\"labels\"])\n",
        "    return inputs\n",
        "'''"
      ],
      "metadata": {
        "id": "_VZv8LqV43mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_dataset[\"train\"][0][\"labels\"], type(processed_dataset[\"train\"][0][\"labels\"]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "acCe6DRS43iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processed_dataset.save_to_disk(\"/content/drive/MyDrive/FinalProject/processed_vilt_dataset\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tG0z_vgkTe3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Apply pre-processing\n",
        "#dataset = dataset.map(preprocess)\n",
        "dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "print(dataset[\"train\"][0][\"labels\"], type(dataset[\"train\"][0][\"labels\"]))\n",
        "'''"
      ],
      "metadata": {
        "id": "awiLnZhP43ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 500 examples\n",
        "medium_dataset = DatasetDict({\n",
        "    \"train\": processed_dataset[\"train\"].select(range(500)),\n",
        "    \"test\": processed_dataset[\"test\"].select(range(100)),\n",
        "})\n"
      ],
      "metadata": {
        "id": "yy_3RG5d5QQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "\n",
        "from transformers import ViltForQuestionAnswering\n",
        "import os  # <-- missing import\n",
        "\n",
        "# Define the base directory (your dataset folder in Google Drive)\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "\n",
        "# Load the list of all possible answers\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    answer_space = f.read().splitlines()"
      ],
      "metadata": {
        "id": "E1tVy4Tm5QNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPIxeztfrUEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltConfig\n",
        "\n",
        "# Step 1: Patch config\n",
        "custom_config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "custom_config.max_position_embeddings = 384\n",
        "custom_config.num_labels = len(answer_space)\n",
        "custom_config.problem_type = \"single_label_classification\"\n"
      ],
      "metadata": {
        "id": "WGyokL2c5QKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Load with override for embedding mismatch\n",
        "model = ViltForSingleLabelQA.from_pretrained(\n",
        "    \"dandelin/vilt-b32-mlm\",\n",
        "    config=custom_config,\n",
        "    ignore_mismatched_sizes=True  # âœ… crucial\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZdZaKpG9Xvf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize position embeddings\n",
        "old_pos_embed = model.vilt.embeddings.position_embeddings\n",
        "new_pos_embed = torch.nn.Parameter(torch.zeros(1, 384, model.config.hidden_size))\n",
        "\n",
        "# Copy pretrained values for first 40\n",
        "new_pos_embed.data[:, :old_pos_embed.shape[1], :] = old_pos_embed.data\n",
        "\n",
        "# Init rest\n",
        "torch.nn.init.trunc_normal_(new_pos_embed[:, 40:], std=0.02)\n",
        "\n",
        "# Replace\n",
        "model.vilt.embeddings.position_embeddings = new_pos_embed\n",
        "\n",
        "# âœ… Sanity check\n",
        "print(model.vilt.embeddings.position_embeddings.shape)\n"
      ],
      "metadata": {
        "id": "FTXkKoHGngIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New position embedding shape:\", model.vilt.embeddings.position_embeddings.shape)"
      ],
      "metadata": {
        "id": "ao7QWS9Yodtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode IDs into answer strings\n",
        "    pred_answers = [answer_space[p] for p in preds]\n",
        "    true_answers = [answer_space[l] for l in labels]\n",
        "\n",
        "    # WUPS across all pairs\n",
        "    wups = [\n",
        "        wup_measure(pred, true)\n",
        "        for pred, true in zip(pred_answers, true_answers)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"wups\": np.mean(wups)\n",
        "    }"
      ],
      "metadata": {
        "id": "NvA8KT7F5QE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel\n",
        "from transformers import logging\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vilt_daquar_output\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",  # try eval_strategy?\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        "    #label_smoothing_factor=0.1,  # âœ… Try 0.1 to start\n",
        "    remove_unused_columns=False  # âœ… ADD THIS LINE\n",
        ")\n"
      ],
      "metadata": {
        "id": "C7mytMBw5QCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edFCUen75P-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "def data_collator(features):\n",
        "    batch = default_data_collator(features)\n",
        "    batch[\"labels\"] = batch[\"labels\"].long()  # ðŸ’¥ force long tensor\n",
        "    return batch"
      ],
      "metadata": {
        "id": "s6mnUwFB43Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    eval_dataset=processed_dataset[\"test\"],\n",
        "    tokenizer=processor,\n",
        "    data_collator=data_collator,  # <-- custom one\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "ZFFoUkvR43W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vilt.embeddings.position_embeddings.shape)"
      ],
      "metadata": {
        "id": "AD5h4rHzUy-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''model.config.problem_type = \"single_label_classification\"\n",
        "\n",
        "# Fix position embedding size for 384 tokens\n",
        "model.config.max_position_embeddings = 384\n",
        "model.vilt.embeddings.position_embeddings = torch.nn.Parameter(\n",
        "    torch.zeros(1, 384, model.config.hidden_size)\n",
        ")\n",
        "torch.nn.init.trunc_normal_(model.vilt.embeddings.position_embeddings, std=0.02)\n",
        "'''"
      ],
      "metadata": {
        "id": "I7nPvNl143T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vilt.embeddings.position_embeddings.shape)\n"
      ],
      "metadata": {
        "id": "iI960wZ8pSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-vaMR1v543P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")"
      ],
      "metadata": {
        "id": "ZI20qXiZ4TCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"/content/drive/MyDrive/FinalProject/vilt_preprocessed\")"
      ],
      "metadata": {
        "id": "szIP8uGzg1xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "with open(\"/content/drive/MyDrive/FinalProject/metrics.json\", \"w\") as f:\n",
        "    import json\n",
        "    json.dump(metrics, f, indent=2)"
      ],
      "metadata": {
        "id": "lM5NUgmog6Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_parameter_count(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"ðŸ§  Total parameters:     {total:,}\")\n",
        "    print(f\"ðŸŽ¯ Trainable parameters: {trainable:,}\")\n",
        "    print(f\"ðŸª¶ Frozen parameters:    {total - trainable:,}\")\n",
        "\n",
        "print_parameter_count(model)"
      ],
      "metadata": {
        "id": "0m44Afizg6Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-SYdqkYg6F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0O7i2FSzg6Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-GEVCdig59b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}