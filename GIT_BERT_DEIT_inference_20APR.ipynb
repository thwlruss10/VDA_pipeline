{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUBL5JCCrv8qBEeldeB4Fo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thwlruss10/VDA_pipeline/blob/main/GIT_BERT_DEIT_inference_20APR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m6kUuU6Uwz_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets nltk scikit-learn"
      ],
      "metadata": {
        "id": "328WIHarJAFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXGmbzDYJ1sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from typing import Optional\n",
        "\n",
        "'''\n",
        "The MultimodalVQAModel takes a question and an image as input.\n",
        "It uses pre-trained models to extract features from both, combines these features,\n",
        "and then predicts the answer to the question based on the combined information.\n",
        "This architecture is typical for VQA tasks, where the goal is to understand both\n",
        "visual and textual information to provide accurate answers.\n",
        "\n",
        "forward() Defines how the model processes data during training and inference.\n",
        "  input_ids: Tokenized representation of the question.\n",
        "  pixel_values: Processed image data.\n",
        "  attention_mask: Indicates which tokens in the question are relevant.\n",
        "  token_type_ids: Distinguishes between different segments of text (if applicable).\n",
        "  labels: The correct answer (used during training).\n",
        "'''\n",
        "\n",
        "class MultimodalVQAModel(nn.Module):  #Defining Mu-Mo-VQA model is per PyTorch base class nn.Module\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_text_name: str,\n",
        "        pretrained_image_name: str,\n",
        "        num_labels: int,\n",
        "        intermediate_dim: int = 512,  # increase to reduce overfitting can modify to 1024\n",
        "        dropout: float = 0.5          # prevents overfitting, can decrease to 0.1.\n",
        "    ):\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "\n",
        "        self.num_labels = num_labels  #length of answer space\n",
        "        self.text_encoder = AutoModel.from_pretrained(pretrained_text_name) # attention mechanism lives in pre-trained transformers\n",
        "        self.image_encoder = AutoModel.from_pretrained(pretrained_image_name)\n",
        "\n",
        "        # Fallback if pooler_output doesn't exist (e.g., in ViT)\n",
        "        self.use_pooler_text = hasattr(self.text_encoder.config, \"pooler_fc_size\")\n",
        "        self.use_pooler_image = hasattr(self.image_encoder.config, \"pooler_fc_size\")\n",
        "\n",
        "        # Fusion + Classification layers\n",
        "        # linear transformation, activation function (ReLU), dropout regularization\n",
        "        # Integrates the combined features comprehensive representation\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size,\n",
        "                intermediate_dim\n",
        "            ),\n",
        "            # nn.BatchNorm1d(intermediate_dim),  # Add Batch Normalization\n",
        "            nn.ReLU(),            # alt nn.LeakyReLU, nn.GELU, or nn.ELU\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    # Feed forward fusion\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None\n",
        "    ):\n",
        "        # Encode text\n",
        "        text_output = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        if \"pooler_output\" in text_output:          # not all pre-trained models have pooling, BERT has\n",
        "            text_feat = text_output.pooler_output   # model to handle different pre-trained encoders by checking\n",
        "        else:\n",
        "            text_feat = text_output.last_hidden_state[:, 0]  # CLS token fallback\n",
        "\n",
        "        # Encode image\n",
        "        image_output = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        if \"pooler_output\" in image_output:\n",
        "            image_feat = image_output.pooler_output\n",
        "        else:\n",
        "            image_feat = image_output.last_hidden_state[:, 0]  # CLS token fallback\n",
        "\n",
        "        # Fuse + classify\n",
        "        fused = self.fusion(torch.cat([text_feat, image_feat], dim=1))\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        output = {\"logits\": logits}\n",
        "        if labels is not None:\n",
        "            output[\"loss\"] = self.criterion(logits, labels)\n",
        "\n",
        "        return output # Returns logit values associated with each label\n"
      ],
      "metadata": {
        "id": "8WAiEgWGy6Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assign device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load config\n",
        "config = torch.load(\"/content/drive/MyDrive/FinalProject/myenv/Demo-VQA_Bert-DEiT_config.pt\")\n",
        "\n",
        "# Reconstruct model\n",
        "model = MultimodalVQAModel(**config)"
      ],
      "metadata": {
        "id": "F4kqeaYNJTkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import ViltForQuestionAnswering, ViltProcessor\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")\n",
        "processor = ViltProcessor.from_pretrained(\"/content/drive/MyDrive/FinalProject/vilt_checkpoint\")\n",
        "'''\n"
      ],
      "metadata": {
        "id": "2OePLCrvw0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory pointing to your Drive location\n",
        "base_dir = \"/content/drive/MyDrive/FinalProject/dataset\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Define regex to extract image ID\n",
        "image_pattern = re.compile(r\"( (in |on |of )?(the |this )?(image\\d*) \\?)\")\n",
        "\n",
        "# Read the raw Q&A file\n",
        "# Extracts image ID and question at index[i] together with answer at [i+1]\n",
        "# and organizes it into a pandas DataFrame, creates a list of unique answers,\n",
        "# and then splits the dataset into training and testing sets, saving them as\n",
        "# CSV.\n",
        "\n",
        "qa_file = os.path.join(base_dir, \"all_qa_pairs.txt\")\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as f: # open in read mode with in utf-8 encoding\n",
        "    qa_data = [x.strip() for x in f.readlines()] # x.strip() removes white spaces\n",
        "\n",
        "records = []\n",
        "for i in range(0, len(qa_data), 2): # iterates in steps of two because quesiton is at index i and answer at i+1\n",
        "    match = image_pattern.findall(qa_data[i]) # extract image ID from question string\n",
        "    if match:\n",
        "        img_id = match[0][3]\n",
        "        question = qa_data[i].replace(match[0][0], \"\").strip() # remove image ID and replace question\n",
        "        answer = qa_data[i + 1].strip() # answer retrieved from next element\n",
        "        records.append({\"question\": question, \"answer\": answer, \"image_id\": img_id}) #populate dictionary\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "\n",
        "answer_space = []\n",
        "for ans in df[\"answer\"].to_list():\n",
        "    if \",\" in ans:\n",
        "        answer_space += ans.replace(\" \", \"\").split(\",\")\n",
        "    else:\n",
        "        answer_space.append(ans)\n",
        "\n",
        "# sort and remove duplicates from answer_space\n",
        "answer_space = sorted(set(answer_space))\n",
        "\n",
        "# Write the answer space to file\n",
        "with open(os.path.join(base_dir, \"answer_space.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(\"\\n\".join(answer_space))\n",
        "\n",
        "# Split dataset into training and evaluation\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save to CSV\n",
        "train_df.to_csv(os.path.join(base_dir, \"data_train.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(base_dir, \"data_eval.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "_clxpE2PAlwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models & transformers (ViTModel, DeiTModel, BEiT)\n",
        "    AutoModel,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "# SET ONLY 1 GPU DEVICE\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "#set_caching_enabled(True)>> this line deleted as 'datasets' no longer supports explicit cahce enabling\n",
        "#set_caching_enabled(True)\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "FXRlfZAvJQxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wup_measure(pred, gt, threshold=0.9):\n",
        "    pred_synsets = wn.synsets(pred)\n",
        "    gt_synsets = wn.synsets(gt)\n",
        "\n",
        "    if not pred_synsets or not gt_synsets:\n",
        "        return 0.0\n",
        "\n",
        "    max_score = max((s1.wup_similarity(s2) or 0) for s1 in pred_synsets for s2 in gt_synsets)\n",
        "    return 1.0 if max_score >= threshold else max_score"
      ],
      "metadata": {
        "id": "YbiNiQEtPSaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n"
      ],
      "metadata": {
        "id": "9kmDWEWTL-_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoImageProcessor\n",
        "\n",
        "# Rebuild processors from original names\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/deit-base-patch16-224\")\n"
      ],
      "metadata": {
        "id": "4Rfi2USGQ4xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/FinalProject/myenv/tokenizer/\")\n",
        "image_processor.save_pretrained(\"/content/drive/MyDrive/FinalProject/myenv/image_processor/\")\n"
      ],
      "metadata": {
        "id": "F0lUEoiiQ7L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoImageProcessor\n",
        "\n",
        "# Load back\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/FinalProject/myenv/tokenizer/\")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"/content/drive/MyDrive/FinalProject/myenv/image_processor/\")\n"
      ],
      "metadata": {
        "id": "MO4spRysRA0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showExample(train=True, id=None):\n",
        "    if train:\n",
        "        data = dataset[\"train\"]\n",
        "    else:\n",
        "        data = dataset[\"test\"]\n",
        "\n",
        "    if id is None:\n",
        "        id = np.random.randint(len(data))\n",
        "\n",
        "    img_path = os.path.join(image_dir, data[id][\"image_id\"] + \".png\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"⚠️ Image not found at: {img_path}\")\n",
        "        return\n",
        "\n",
        "    label = answer_space.index(data[id][\"answer\"]) if data[id][\"answer\"] in answer_space else \"N/A\"\n",
        "\n",
        "    # Load question and image\n",
        "    question = data[id][\"question\"]\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # 🔠 Tokenize question\n",
        "    text_inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # 🖼️ Process image\n",
        "    image_inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # 🧩 Merge inputs\n",
        "    inputs = {\n",
        "        \"input_ids\": text_inputs[\"input_ids\"],\n",
        "        \"attention_mask\": text_inputs[\"attention_mask\"],\n",
        "        \"pixel_values\": image_inputs[\"pixel_values\"]\n",
        "    }\n",
        "\n",
        "    # 🔮 Run model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # 📈 Predict\n",
        "    logits = outputs[\"logits\"]  # fixed: dictionary-style access\n",
        "    predicted_idx = logits.argmax(-1).item()\n",
        "    predicted_answer = answer_space[predicted_idx]\n",
        "    wup_score = wup_measure(pred=predicted_answer, gt=data[id][\"answer\"])\n",
        "\n",
        "    # 🎨 Display result\n",
        "    print(f\"✅ Found image at: {os.path.abspath(img_path)}\")\n",
        "    display(image)\n",
        "    print(\"Question:\\t\", question)\n",
        "    label = data[id].get(\"label\", \"N/A\")  # fallback if key is missing\n",
        "    #print(\"Answer:\\t\\t\", data[id][\"answer\"], f\"(Label: {label})\")\n",
        "    print(\"Answer:\\t\\t\", data[id][\"answer\"])\n",
        "    print(\"Predicted answer:\", predicted_answer)\n",
        "    print(\"WUP Score:\", wup_score)\n"
      ],
      "metadata": {
        "id": "FLXU47qnSl-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showExample()"
      ],
      "metadata": {
        "id": "_1Nl7KP7Snz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set the correct image directory in Google Drive\n",
        "image_dir = \"/content/drive/MyDrive/FinalProject/dataset/images\"\n",
        "\n",
        "def showExample(train=True, id=None):\n",
        "    if train:\n",
        "        data = dataset[\"train\"]\n",
        "    else:\n",
        "        data = dataset[\"test\"]\n",
        "\n",
        "    if id is None:\n",
        "        id = np.random.randint(len(data))\n",
        "\n",
        "    img_path = os.path.join(image_dir, data[id][\"image_id\"] + \".png\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"⚠️ Image not found at: {img_path}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Define question and image path\n",
        "    question = data[id][\"question\"]\n",
        "    #  image_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image3.png\"\n",
        "\n",
        "    # Load image as RGB PIL object\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Preprocess inputs\n",
        "    inputs = image_processor(text=question, images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Run model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get predicted answer index and label\n",
        "    logits = outputs.logits\n",
        "    predicted_idx = logits.argmax(-1).item()\n",
        "    predicted_answer = answer_space[predicted_idx]\n",
        "    wup_score = wup_measure(pred=predicted_answer, gt=data[id][\"answer\"])\n",
        "\n",
        "    print(f\"✅ Found image at: {os.path.abspath(img_path)}\")\n",
        "    #  image = Image.open(img_path)\n",
        "    display(image)\n",
        "\n",
        "    print(\"Question:\\t\", question)\n",
        "    print(\"Answer:\\t\\t\", data[id][\"answer\"], f\"(Label: {data[id]['label']})\")\n",
        "    print(\"Predicted answer:\", predicted_answer)\n",
        "    print(\"WUP Score:\", wup_score)\n",
        "\n",
        "# Show a random training example\n",
        "showExample()"
      ],
      "metadata": {
        "id": "AH9xv-jvIkXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "\n",
        "# Load your trained model and processor\n",
        "# model = ViltForQuestionAnswering.from_pretrained(\"path_to_your_saved_model\")\n",
        "# processor = ViltProcessor.from_pretrained(\"path_to_your_saved_model\")\n",
        "\n",
        "# Load answer space from file\n",
        "answer_space_path = \"/content/drive/MyDrive/FinalProject/dataset/answer_space.txt\"\n",
        "with open(answer_space_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    answer_space = f.read().splitlines()\n",
        "\n",
        "# Define question and image path\n",
        "question = \"What is the color of the chair in front of the white wall?\"\n",
        "image_path = \"/content/drive/MyDrive/FinalProject/dataset/images/image3.png\"\n",
        "\n",
        "# Load image as RGB PIL object\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess inputs\n",
        "inputs = image_processor(text=question, images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Run model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get predicted answer index and label\n",
        "logits = outputs.logits\n",
        "predicted_idx = logits.argmax(-1).item()\n",
        "predicted_answer = answer_space[predicted_idx]\n",
        "\n",
        "print(\"Predicted answer:\", predicted_answer)\n"
      ],
      "metadata": {
        "id": "_-kA5RwgxYQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVllnQUCHxMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}